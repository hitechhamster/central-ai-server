# main.py
from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from starlette.middleware.base import BaseHTTPMiddleware
from contextlib import asynccontextmanager
import os
import httpx
import json
from dotenv import load_dotenv
from collections import defaultdict
import time
from typing import Optional

# âœ… å¯¼å…¥æ•°æ®åº“æ¨¡å—
from database import init_database, save_conversation, get_recent_conversations, get_stats, search_conversations

# --- Lifespan ç®¡ç†ï¼ˆæ›¿ä»£å¼ƒç”¨çš„ on_eventï¼‰---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶
    init_database()
    print("âœ… æ•°æ®åº“å·²åˆå§‹åŒ–")
    print("âœ… AI æœåŠ¡å¯åŠ¨æˆåŠŸ")
    yield
    # å…³é—­æ—¶
    print("ğŸ‘‹ AI æœåŠ¡å…³é—­")

# --- åˆå§‹åŒ–åº”ç”¨ ---
app = FastAPI(title="Central AI Service", lifespan=lifespan)

# --- è‡ªå®šä¹‰ CORS ä¸­é—´ä»¶ï¼ˆåªå…è®¸ Shopify åŸŸåï¼‰---
class CustomCORSMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        origin = request.headers.get("origin", "")
        
        # æ£€æŸ¥ç¯å¢ƒ
        is_production = os.environ.get("ENVIRONMENT") == "production"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…è®¸çš„åŸŸå
        allowed = (
            origin.endswith(".myshopify.com") or 
            origin == "https://admin.shopify.com" or
            origin == "https://theqiflow.com" or
            origin == "https://fengshuisource.com" or
            (not is_production and origin.startswith("http://localhost"))
        )
        
        # å¤„ç†é¢„æ£€è¯·æ±‚ï¼ˆOPTIONSï¼‰
        if request.method == "OPTIONS":
            if allowed:
                return Response(
                    status_code=200,
                    headers={
                        "Access-Control-Allow-Origin": origin,
                        "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
                        "Access-Control-Allow-Headers": "Content-Type",
                        "Access-Control-Allow-Credentials": "true",
                    }
                )
            return Response(status_code=403)
        
        # å¤„ç†æ­£å¸¸è¯·æ±‚
        response = await call_next(request)
        
        if allowed:
            response.headers["Access-Control-Allow-Origin"] = origin
            response.headers["Access-Control-Allow-Credentials"] = "true"
        
        return response

app.add_middleware(CustomCORSMiddleware)

# --- é€Ÿç‡é™åˆ¶é…ç½® ---
minute_requests = defaultdict(lambda: {"count": 0, "reset_time": 0})
MINUTE_LIMIT = 10

daily_requests = defaultdict(lambda: {"count": 0, "reset_time": 0})
DAILY_LIMIT = 100

def get_client_ip(request: Request) -> str:
    """è·å–å®¢æˆ·ç«¯çœŸå® IP åœ°å€"""
    forwarded = request.headers.get("X-Forwarded-For")
    if forwarded:
        return forwarded.split(",")[0].strip()
    real_ip = request.headers.get("X-Real-IP")
    if real_ip:
        return real_ip
    return request.client.host

def check_rate_limit(ip: str):
    """æ£€æŸ¥é€Ÿç‡é™åˆ¶ï¼Œè¿”å›: (æ˜¯å¦å…è®¸è¯·æ±‚, é”™è¯¯ä¿¡æ¯)"""
    now = time.time()
    
    # æ£€æŸ¥æ¯åˆ†é’Ÿé™åˆ¶
    minute_data = minute_requests[ip]
    if now - minute_data["reset_time"] > 60:
        minute_data["count"] = 0
        minute_data["reset_time"] = now
    
    if minute_data["count"] >= MINUTE_LIMIT:
        return False, f"æ¯åˆ†é’Ÿæœ€å¤š {MINUTE_LIMIT} æ¬¡è¯·æ±‚ï¼Œè¯·ç¨åå†è¯•"
    
    # æ£€æŸ¥æ¯æ—¥é™åˆ¶
    daily_data = daily_requests[ip]
    if now - daily_data["reset_time"] > 86400:
        daily_data["count"] = 0
        daily_data["reset_time"] = now
    
    if daily_data["count"] >= DAILY_LIMIT:
        return False, f"æ¯å¤©æœ€å¤š {DAILY_LIMIT} æ¬¡è¯·æ±‚ï¼Œè¯·æ˜å¤©å†è¯•"
    
    # é€šè¿‡æ£€æŸ¥ï¼Œå¢åŠ è®¡æ•°
    minute_data["count"] += 1
    daily_data["count"] += 1
    
    return True, ""

# --- å¼‚æ­¥ LLM å®¢æˆ·ç«¯ ---
class AsyncLLMClient:
    """å¼‚æ­¥ LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šæ¨¡å‹"""
    
    def __init__(self):
        load_dotenv()
        self.api_key = os.environ.get('OPENROUTER_API_KEY')
        self.default_model = "google/gemini-3-flash-preview"
        
        # âœ… å…è®¸çš„æ¨¡å‹ç™½åå•
        self.allowed_models = {
            "google/gemini-3-flash-preview",   # é»˜è®¤å¿«é€Ÿæ¨¡å‹
            "google/gemini-3-pro-preview",     # é«˜çº§æ¨ç†æ¨¡å‹ï¼ˆç”¨äºå¤æ‚ä»»åŠ¡ï¼‰
        }
    
    def get_model(self, requested_model: Optional[str]) -> str:
        """éªŒè¯å¹¶è¿”å›è¦ä½¿ç”¨çš„æ¨¡å‹"""
        if requested_model and requested_model in self.allowed_models:
            return requested_model
        return self.default_model
    
    async def call_llm(self, prompt_text: str, model: Optional[str] = None) -> str:
        """å¼‚æ­¥è°ƒç”¨ OpenRouter API"""
        if not self.api_key:
            print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° OPENROUTER_API_KEY ç¯å¢ƒå˜é‡")
            return "æœåŠ¡å™¨é…ç½®é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
        
        use_model = self.get_model(model)
        
        try:
            async with httpx.AsyncClient(timeout=180.0) as client:
                response = await client.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                        "HTTP-Referer": "https://theqiflow.com",
                        "X-Title": "Feng Shui AI Service"
                    },
                    json={
                        "model": use_model,
                        "messages": [{"role": "user", "content": prompt_text}],
                        "temperature": 0.7,
                        "max_tokens": 8000
                    }
                )
                response.raise_for_status()
                result = response.json()
                return result['choices'][0]['message']['content']
        
        except httpx.TimeoutException:
            print("â±ï¸ é”™è¯¯ï¼šAPI è¯·æ±‚è¶…æ—¶")
            return "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"
        
        except httpx.HTTPStatusError as e:
            print(f"ğŸš« HTTP é”™è¯¯ï¼š{e.response.status_code} - {e.response.text}")
            return "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
        
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯ï¼š{e}")
            return "æœåŠ¡å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•"

# åˆå§‹åŒ–å¼‚æ­¥ LLM å®¢æˆ·ç«¯
llm_client = AsyncLLMClient()

# --- API æ•°æ®æ¨¡å‹ ---
class AIRequest(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=100000)
    model: Optional[str] = None  # âœ… å¯é€‰ï¼šæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹

# --- API ç«¯ç‚¹ ---
@app.get("/")
def read_root():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "message": "AI Service è¿è¡Œä¸­",
        "status": "ok",
        "version": "2.0",
        "default_model": llm_client.default_model,
        "available_models": list(llm_client.allowed_models)
    }

@app.post("/api/ask-ai")
async def ask_ai_endpoint(request: AIRequest, req: Request):
    """
    AI é—®ç­”ç«¯ç‚¹ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    
    å‚æ•°ï¼š
    - prompt: é—®é¢˜å†…å®¹
    - model: å¯é€‰ï¼ŒæŒ‡å®šæ¨¡å‹ (å¦‚ "google/gemini-2.5-pro-preview")
    """
    # è·å–å®¢æˆ·ç«¯ IP
    client_ip = get_client_ip(req)
    
    # æ£€æŸ¥é€Ÿç‡é™åˆ¶
    allowed, error_msg = check_rate_limit(client_ip)
    if not allowed:
        raise HTTPException(status_code=429, detail=error_msg)
    
    # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
    use_model = llm_client.get_model(request.model)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # âœ… å¼‚æ­¥è°ƒç”¨ AI
    ai_response = await llm_client.call_llm(request.prompt, use_model)
    
    # è®¡ç®—å“åº”æ—¶é—´
    response_time = time.time() - start_time
    
    # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå®¹é”™å¤„ç†ï¼‰
    try:
        save_conversation(
            ip=client_ip,
            prompt=request.prompt,
            response=ai_response,
            model=use_model,
            response_time=response_time
        )
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å¯¹è¯å¤±è´¥: {e}")
    
    # æ‰“å°ä½¿ç”¨æƒ…å†µæ—¥å¿—
    print(f"[{client_ip}] æ¨¡å‹: {use_model} | è€—æ—¶: {response_time:.2f}s | åˆ†é’Ÿ: {minute_requests[client_ip]['count']}/{MINUTE_LIMIT}")
    
    return {"response": ai_response, "model": use_model}

@app.get("/api/health")
def health_check():
    """å¥åº·æ£€æŸ¥ï¼ˆç”¨äºç›‘æ§ï¼‰"""
    return {"status": "healthy", "timestamp": time.time()}

# --- ç®¡ç†ç«¯ç‚¹ï¼ˆæš‚ä¸åŠ è®¤è¯ï¼‰---
@app.get("/admin/stats")
def get_usage_stats():
    """æŸ¥çœ‹å½“å‰ä½¿ç”¨ç»Ÿè®¡"""
    try:
        db_stats = get_stats()
    except:
        db_stats = {"error": "æ•°æ®åº“æš‚æ— æ•°æ®"}
    
    return {
        "database": db_stats,
        "rate_limits": {
            "minute_usage": {
                ip: data["count"] 
                for ip, data in minute_requests.items() 
                if data["count"] > 0
            },
            "daily_usage": {
                ip: data["count"] 
                for ip, data in daily_requests.items() 
                if data["count"] > 0
            }
        }
    }

@app.get("/admin/conversations")
def get_conversations(limit: int = 50):
    """æŸ¥çœ‹æœ€è¿‘çš„å¯¹è¯è®°å½•"""
    try:
        conversations = get_recent_conversations(limit)
        return {
            "total": len(conversations),
            "conversations": conversations
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥: {str(e)}")

@app.get("/admin/search")
def search_conversations_endpoint(keyword: str = None, ip: str = None, limit: int = 50):
    """æœç´¢å¯¹è¯è®°å½•"""
    try:
        results = search_conversations(keyword, ip, limit)
        return {
            "total": len(results),
            "results": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")
